<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Gaurav Ghosal</title>

    <meta name="author" content="Gaurav Ghosal">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Gaurav Ghosal
                </p>
                <p>I'm a second year Ph.D. student at the Carnegie Mellon University Machine Learning Department advised by <a href=https://www.cs.cmu.edu/~aditirag/>Professor Aditi Raghunathan</a>. My interests lie broadly in improving the reliability of foundation and large language models. Some areas I am especially focused on include <b>factuality</b>: ensuring that LLMs learn and correctly apply factual knowledge in downstream tasks, <b>model updating</b>: designing LLMs that can be efficiently and reliably updated with new information without losing their general capabilities, and <b>understanding training dynamics</b>: characterizing how LLM failure modes arise during training and developing principled methods to mitigate them. Prior to CMU, I was an undergraduate student and researcher at UC Berkeley where I worked on human-AI interaction with <a href="https://people.eecs.berkeley.edu/~anca">Anca Dragan</a>.</p>
                </p>
                <p style="text-align:center">
                  <a href="mailto:gauravrghosal@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Gaurav_R_Ghosal_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=SkyPSDUAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/gaurav_ghosal">Twitter</a> &nbsp;/&nbsp;                  <a href="https://github.com/grghosal">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/GauravGhosal.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/GauravGhosal.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  Most of my current research focuses on improving reliability of large language models. I pay special attention to problems in improving LLM <b>factuality</b>, enabling efficient <b>updating</b> and <b>unlearning</b>, and studying LLM <b>training dynamics</b> to better understand the origin of their failure modes. In my undergraduate, I published papers on <b>learning from human feedback</b> and <b>robustness to spurious correlations</b>.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/UnderstandingFinetuningForFactualKnowledgeExtracton.png" alt="ftknowlkedgeextraction" width="280" height="200">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2406.14785">
                  <span class="papertitle">Understanding Finetuning for Factual Knowledge Extraction</span>
                </a>
                <br>
                <strong>Gaurav R. Ghosal</strong>, <a href="https://thashim.github.io/">Tatsunori Hashimoto</a>, <a href="https://www.cs.cmu.edu/~aditirag">Aditi Raghunathan</a>
                <br>
                <em>International Conference on Machine Learning</em>, 2024
                <p>Large language models are exposed to a large amount of factual knowledge during pretraining. How can we ensure that this knowledge is applied in downstream tasks? We make the surprising discovery that finetuning on facts the <em>model knows well ensures</em> leads to better factuality downstream. We substantiate our findings via a theoretical analysis of fine-tuning dynamics. </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/ContextualReliability.png" alt="ftknowlkedgeextraction" width="275" height="125">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2307.10026">
                  <span class="papertitle">Contextual Reliability: When Different Features Matter in Different
                    Contexts</span>
                </a>
                <br>
                <strong>Gaurav R. Ghosal</strong>, <a href="https://github.com/ars22">Amrith Setlur</a>, <a href="https://users.cs.utah.edu/~dsbrown/">Daniel S. Brown</a>, <a href="https://people.eecs.berkeley.edu/~anca/">Anca Dragan</a>, <a href="https://www.cs.cmu.edu/~aditirag">Aditi Raghunathan</a>
                <br>
                <em>International Conference on Machine Learning</em>, 2023
                <p>In many real-world settings, such as autonomous driving, a feature can be important to pay attention to in one context (i.e. a crosswalk), but a distractor in another (a traffic light). How can we make sure that deep neural networks rely on the right features in the right context (without using distractors)? We propose a two stage training method that leverages a limited set of <em>human annotations</em> to enforce robustness. We validate our proposed method on the real-world OpenWaymo challenge. </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:20%;vertical-align:middle">
                <img src="images/FigureV7.png" alt="ftknowlkedgeextraction" width="270" height="150">
              </td>
              <td style="padding:8px;width:80%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2208.10687">
                  <span class="papertitle">The Effect of Modeling Human Rationality Level on Learning Rewards from Multiple Feedback Types</span>
                </a>
                <br>
                <strong>Gaurav R. Ghosal</strong>, Matthew Zurek, <a href="https://users.cs.utah.edu/~dsbrown/">Daniel S. Brown</a>, <a href="https://people.eecs.berkeley.edu/~anca/">Anca Dragan</a>
                <br>
                <em>AAAI</em>, 2023
                <p> Inferring desired behavior from users is critical for safe and reliable AI, however, users often can give noisy or systematically biased feedback. In this work, we study the effect of <em>modeling</em> the user rationality level when learning a reward function from human feedback. We find that modelling rationality level can be especially desirable in <b>active</b> multi-feedback-type reward learning settings: different types of feedback queries can be optimal depending on the rationality level.</p>
              </td>
            </tr>


            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #c6b89e;">
								 <h2>Academic Service</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://icml.cc/">Reviewer, ICML 2025</a>
                <br>

                <a href="https://iclr.cc/">Reviewer, ICLR 2025</a>
                <br>
                <a href="https://www.cs.cmu.edu/academics/phd/application-support">Graduate Application Mentor, CMU GASP (2024-2025)</a>
                <br>
                <a href="https://sites.google.com/andrew.cmu.edu/ai-mentoring/">Undergraduate Mentor, CMU PAIR (2024-2025)</a>"
                <br>
                <a href="https://aaai.org/conference/aaai/aaai-25/">Reviewer, AAAI 2025</a>
                <br>
              </td>
            </tr>
						
						
           
            <tr>
              <td align="center" style="padding:16px;width:20%;vertical-align:middle">
						     <div class="colored-box" style="background-color: #edd892;">
								 <h2>Teaching</h2>
								 </div>
              </td>
              <td style="padding:8px;width:80%;vertical-align:center">
                <a href="https://people.eecs.berkeley.edu/~jrs/189/">Undergraduate Student Instructor, CS189 Spring 2022</a>
                <br>
              </td>
            </tr>
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Credit to Jon Barron for the website template <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
